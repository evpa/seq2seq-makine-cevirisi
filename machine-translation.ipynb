{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makine Çevirisi\n",
    "\n",
    "Bu notebook'ta Tatoeba'da yapılmış İngilizce-Türkçe çevirileri kullanarak İngilizce-Türkçe çeviri yapabilen bir model geliştireceğiz. Kullanılan veriseti aşağıdadır:\n",
    "\n",
    "http://www.manythings.org/anki/\n",
    "\n",
    "Oluşturacağımız model aşağıdaki diyagramdaki gibi olacak. Model iki parçadan oluşuyor. İngilizce cümleleri işleyecek ilk parçaya encoder diyoruz. Türkçe cümleler üretecek ikinci parçaya ise decoder denir. İlk önce encoder gelen cümleyi işler ve bir vektör üretir. Bu vektörü düşünce vektörü olarak düşünebiliriz. Encoder aldığı cümleyi işliyor ve özetini bir vektöre yazıyor. Ardından üretilen vektörü decoder alır ve Türkçe kelimeler üretmeye başlar. Eğer model iyi eğitilmişse üretilen Türkçe cümle verdiğimiz İngilizce cümlenin çevirisi olacaktır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Flowchart](diag/nmt-diag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maynard\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding, CuDNNGRU\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder'un kelime üretmeye başlaması için öncesinde bir kelime vermemiz gerekir. Aldığı kelimeden sonra üretime başlayacaktır. Bunun için ise verisetinde bulunmayan 'ssss' kullanacağız. 'ssss' tokenini decoder'a verdiğimizde üretime başlayacaktır. Bir de üretimi sonlandırması için end tokenimiz var. Bunu ise yine verisetinde olmayan 'eeee' olarak belirliyoruz. Decoder bu tokeni ürettiği zaman kelime üretimi sonlanacak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_start = 'ssss '\n",
    "mark_end = ' eeee'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder ve decoder'a vereceğimiz veriler için liste oluşturup cümleleri bu listeye yazıyoruz. data_src'da İngilizce cümleler data_dest'te Türkçe cümleler bulunacak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_src = []\n",
    "data_dest = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in open('data/tur.txt', encoding='UTF-8'):\n",
    "    en_text, tr_text = line.rstrip().split('\\t')\n",
    "\n",
    "    tr_text = mark_start + tr_text + mark_end\n",
    "    \n",
    "    data_src.append(en_text)\n",
    "    data_dest.append(tr_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Örnek İngilizce cümle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I drove.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_src[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ve bu cümlenin verisetindeki Türkçe çevirisi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss Araba sürdüm. eeee'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dest[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can you see anything missing?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_src[200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss Eksik bir şey görebiliyor musun? eeee'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dest[200000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toplam veri sayısı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "473035"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenleştirme\n",
    "\n",
    "Keras'ın Tokenizer'ında ihtiyacımız olan birkaç fonksiyon daha var. O yüzden bir wrapper oluşturuyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrap(Tokenizer):\n",
    "    \n",
    "    def __init__(self, texts, padding, reverse=False, num_words=None):\n",
    "        Tokenizer.__init__(self, num_words=num_words)\n",
    "\n",
    "        self.fit_on_texts(texts)\n",
    "\n",
    "        self.index_to_word = dict(zip(self.word_index.values(),\n",
    "                                      self.word_index.keys()))\n",
    "\n",
    "        self.tokens = self.texts_to_sequences(texts)\n",
    "\n",
    "        if reverse:\n",
    "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "\n",
    "        self.num_tokens = [len(x) for x in self.tokens]\n",
    "        self.max_tokens = np.mean(self.num_tokens) + 2 * np.std(self.num_tokens)\n",
    "        self.max_tokens = int(self.max_tokens)\n",
    "\n",
    "        self.tokens_padded = pad_sequences(self.tokens,\n",
    "                                           maxlen=self.max_tokens,\n",
    "                                           padding=padding,\n",
    "                                           truncating=truncating)\n",
    "\n",
    "    def token_to_word(self, token):\n",
    "        word = \" \" if token == 0 else self.index_to_word[token]\n",
    "        return word \n",
    "\n",
    "    def tokens_to_string(self, tokens):\n",
    "        words = [self.index_to_word[token] for token in tokens if token != 0]\n",
    "        \n",
    "        text = \" \".join(words)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def text_to_tokens(self, text, padding, reverse=False):\n",
    "        tokens = self.texts_to_sequences([text])\n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "        if reverse:\n",
    "            tokens = np.flip(tokens, axis=1)\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "\n",
    "        tokens = pad_sequences(tokens,\n",
    "                               maxlen=self.max_tokens,\n",
    "                               padding=padding,\n",
    "                               truncating=truncating)\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_src'daki İngilizce cümleleri tokenleştiriyoruz. Padding'de pre kullandığımız için eksik cümlelerde cümlelerin başına sıfır eklenecek. Ayrıca reverse=True ile cümleyi ters çeviriyoruz. Bu sayede encoder'ın gördüğü son kelime ile decoder'ın üreteceği ilk cümle eşleşecektir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_src = TokenizerWrap(texts=data_src,\n",
    "                              padding='pre',\n",
    "                              reverse=True,\n",
    "                              num_words=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Burada ise Türkçe cümleleri tokenleştiriyoruz. Bu sefer padding'de post kullanıyoruz. Yani eksik cümlelerde cümlenin sonuna sıfır sıfır eklenecek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dest = TokenizerWrap(texts=data_dest, \n",
    "                               padding='post',\n",
    "                               reverse=False,\n",
    "                               num_words=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenleştirme işlemini gerçekleştiriyoruz. Sonuç olarak bütün İngilizce cümleler 11 tokenden ve büütün Türkçe cümleler ise 10 tokenden oluşuyor. RNN'e verdiğimiz inputların boyutu hep aynı olması gerektiği için her cümleyi eşitledik. Cümlede eksik kelimeler varsa sıfır eklenecek, fazla kelimeler varsa cümle kesilecek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(473035, 11)\n",
      "(473035, 10)\n"
     ]
    }
   ],
   "source": [
    "tokens_src = tokenizer_src.tokens_padded\n",
    "tokens_dest = tokenizer_dest.tokens_padded\n",
    "print(tokens_src.shape)\n",
    "print(tokens_dest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Örnekte tokenleri görebilirsiniz. Cümle 10 kelimeden kısa olduğu için sonuna üç tane sıfır eklendi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1, 2391,    4,   18, 4127,   48,    2,    0,    0,    0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_dest[200000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aynı cümleyi yazı olarakta görebiliriz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss eksik bir şey görebiliyor musun eeee'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dest.tokens_to_string(tokens_dest[200000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu da aynı cümlenin İngilizcesi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0, 1028,  113,   95,    5,   39])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_src[200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'missing anything see you can'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_src.tokens_to_string(tokens_src[200000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aynı zamanda verisetindeki orijinal halinede bakabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can you see anything missing?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_src[200000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'ssss' kelimesiyle belirlediğimiz başlangıç tokeni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_start = tokenizer_dest.word_index[mark_start.strip()]\n",
    "token_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'eeee' kelimesi ile belirlediğimiz bitiş tokeni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_end = tokenizer_dest.word_index[mark_end.strip()]\n",
    "token_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "Encoder için input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = tokens_src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder için input ve output. Output inputun bir kaydırılmış hali olacak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_data = tokens_dest[:, :-1]\n",
    "decoder_output_data = tokens_dest[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input ve output örnekleri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0, 1028,  113,   95,    5,   39])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1, 2391,    4,   18, 4127,   48,    2,    0,    0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2391,    4,   18, 4127,   48,    2,    0,    0,    0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data[200000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String'e çevrilmiş hali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss eksik bir şey görebiliyor musun eeee'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dest.tokens_to_string(decoder_input_data[200000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eksik bir şey görebiliyor musun eeee'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dest.tokens_to_string(decoder_output_data[200000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bir de toplamda kaç kelimemiz var bakalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_words = len(tokenizer_src.word_index)\n",
    "num_decoder_words = len(tokenizer_dest.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21315 tane İngilizce kelimemiz var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21315"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_encoder_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "94058 tane de Türkçe kelimemiz var. Türkçe sondan eklemeli bir dil olduğu için İngilizce'ye göre çok daha fazla kelime bulunuyor. Modelimiz için \"ev\" ve \"eve\" kelimeleri tamamen farklı iki kelime olacaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94058"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decoder_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "Kelimeler için 100 uzunluğunda vektörler kullanacağız. Vektörler için Stonford Üniversitesi'nin hazır eğitilmiş vektörleri kullanacağız. Kullandığımız vektörler GLoVe ile Wikipedia üzerinde 6 milyar kelime üzerinde eğitilmiş. Tam ismi \"glove.6B.100d.txt\".\n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLoVe ile eğitilmiş vektörleri verisetindeki vektörler ile karşılaştırıyoruz. Eğer verisetimizdeki bir kelimenin GLoVe vektörü bulunmuyorsa vektör rastgele sayılardan oluşacak. Sadece İngilizce için hazır eğitilmiş vektör kullanacağız. Türkçe için vektörler rastgele başlayıp eğitim sırasında eğitilecek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = {}\n",
    "with open('glove.6B.100d.txt', encoding='UTF-8') as f:\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    word2vec[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.uniform(-1, 1, (num_encoder_words, embedding_size))\n",
    "for word, i in tokenizer_src.word_index.items():\n",
    "    if i < num_encoder_words:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding matrisimizin boyutu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21315, 100)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder için input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = Input(shape=(None, ), name='encoder_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder için Embedding layer. Daha önce topladığımız vektörleri weights parametresine atıyoruz. trainable'a True vererek bu vektörleri eğitileilir yapıyoruz. Modeli eğitirken aynı zamanda embedding layer'da eğitilecek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_embedding = Embedding(input_dim=num_encoder_words,\n",
    "                              output_dim=embedding_size,\n",
    "                              weights=[embedding_matrix],\n",
    "                              trainable=True,\n",
    "                              name='encoder_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state_size ile sinir ağımızdaki nöronları belirliyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder için GRU layer'larımız. Toplamda 3 katlı bir sinir ağı oluşturuyoruz. Son layer bir vektör döndüreceği için return_sequences False olması gerekiyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_gru1 = CuDNNGRU(state_size, name='encoder_gru1', return_sequences=True)\n",
    "encoder_gru2 = CuDNNGRU(state_size, name='encoder_gru2', return_sequences=True)\n",
    "encoder_gru3 = CuDNNGRU(state_size, name='encoder_gru3', return_sequences=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oluşturduğumuz encoder layer'larını birbirine bağlayacak bir fonksiyon yazıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_encoder():\n",
    "    net = encoder_input\n",
    "    \n",
    "    net = encoder_embedding(net)\n",
    "\n",
    "    net = encoder_gru1(net)\n",
    "    net = encoder_gru2(net)\n",
    "    net = encoder_gru3(net)\n",
    "\n",
    "    encoder_output = net\n",
    "    \n",
    "    return encoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonksiyonu çağırarak encoder'ı oluşturalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = connect_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Decoder için iki tane input layer'ımız olması gerekiyor. Bir input layer ile decoder'ın ürettiği vektörü decoder'a vereceğiz diğer input layer ile ise Türkçe cümleleri decoder'a vereceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_initial_state = Input(shape=(state_size,), name='decoder_initial_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(None, ), name='decoder_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder için embedding. Burada hazır eğitilmiş vektörler kullanmıyoruz. Vektörler rastgele oluşturulacak ve eğitim aşamasında eğitilecek. Word2vec veya GLoVe ile eğitilmiş kelime vektörleri kullanılırsa model daha başarılı olacaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_embedding = Embedding(input_dim=num_decoder_words,\n",
    "                              output_dim=embedding_size,\n",
    "                              name='decoder_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder için GRU layer'larımız. Decoder'da her zaman sequence ürettiğimiz için return_sequences True olmalı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_gru1 = CuDNNGRU(state_size, name='decoder_gru1', return_sequences=True)\n",
    "decoder_gru2 = CuDNNGRU(state_size, name='decoder_gru2', return_sequences=True)\n",
    "decoder_gru3 = CuDNNGRU(state_size, name='decoder_gru3', return_sequences=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son olarak decoder'da bir dense(fully connected) layer oluşturuyoruz. Bu layer'da üretilecek olan kelimeye karar vereceğiz. Aktivasyona softmax yerine liner dedik. Biraz sonra loss fonksiyonu yazdığımızda softmax işlemi loss fonksiyonunda yapılacak. O yüzden liner vermemiz gerekiyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dense = Dense(num_decoder_words,\n",
    "                      activation='linear',\n",
    "                      name='decoder_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder layer'larını bağlayacak fonksiyon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_decoder(initial_state):\n",
    "    net = decoder_input\n",
    "\n",
    "    net = decoder_embedding(net)\n",
    "    \n",
    "    net = decoder_gru1(net, initial_state=initial_state)\n",
    "    net = decoder_gru2(net, initial_state=initial_state)\n",
    "    net = decoder_gru3(net, initial_state=initial_state)\n",
    "\n",
    "    decoder_output = decoder_dense(net)\n",
    "    \n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelleri bağlama\n",
    "\n",
    "Burada eğitilecek olan modeli oluşturuyoruz. Modeli end to end eğitebilmek için encoder ve decoder'ı birbirine bağlıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = connect_decoder(initial_state=encoder_output)\n",
    "\n",
    "model_train = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi de sadece encoder için bir model oluşturuyoruz. Bu modeli düşünce vektörü üretmek için kullanacağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_encoder = Model(inputs=[encoder_input], outputs=[encoder_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bir de sadece decoder için model oluşturuyoruz. Bu modele düşünce vektörü vererek kelime üretmesini sağlayabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = connect_decoder(initial_state=decoder_initial_state)\n",
    "\n",
    "model_decoder = Model(inputs=[decoder_input, decoder_initial_state], outputs=[decoder_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss fonksiyonu\n",
    "\n",
    "Loss fonksiyonu için sparse cross entropy kullanıyoruz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cross_entropy(y_true, y_pred):\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer için RMSprop kullanıyoruz. Learning rate olara 0.001 kullanıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target = tf.placeholder(dtype='int32', shape=(None, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derleme\n",
    "\n",
    "Eğitilecek modeli derliyoruz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train.compile(optimizer=optimizer,\n",
    "                    loss=sparse_cross_entropy,\n",
    "                    target_tensors=[decoder_target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eğitim tamamlandıktan sonra modeli kaydedebilmek için checpoint oluşturuyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = 'checkpoint.keras'\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eğer eğitilmiş bir model vaarsa o yüklenecek yoksa sıfırdan eğitime başlanacak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_train.load_weights(path_checkpoint)\n",
    "except Exception as error:\n",
    "    print(\"Checkpoint yüklenirken hata oluştu. Eğitime sıfırdan başlanıyor.\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modele verilecek inputlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = {'encoder_input': encoder_input_data, 'decoder_input': decoder_input_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modele verilecek output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = {'decoder_output': decoder_output_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eğitim\n",
    "\n",
    "Son olarak modeli eğitiyoruz. 256 batch_size ile 10 epoch eğitim yapılacak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maynard\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2119cbea390>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_train.fit(x=x_data,\n",
    "                y=y_data,\n",
    "                batch_size=256,\n",
    "                epochs=10,\n",
    "                callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Çeviri örnekleri\n",
    "\n",
    "Eğitim tamamlandıktan sonra bu fonksiyon ile çeviri yapabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_text, true_output_text=None):\n",
    "    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\n",
    "                                                reverse=True,\n",
    "                                                padding='pre')\n",
    "    \n",
    "    initial_state = model_encoder.predict(input_tokens)\n",
    "    \n",
    "    max_tokens = tokenizer_dest.max_tokens\n",
    "    \n",
    "    decoder_input_data = np.zeros(shape=(1, max_tokens), dtype=np.int)\n",
    "    \n",
    "    token_int = token_start\n",
    "    output_text = ''\n",
    "    count_tokens = 0\n",
    "\n",
    "    while token_int != token_end and count_tokens < max_tokens:\n",
    "        decoder_input_data[0, count_tokens] = token_int\n",
    "        x_data = {'decoder_initial_state': initial_state, 'decoder_input': decoder_input_data}\n",
    "\n",
    "        decoder_output = model_decoder.predict(x_data)\n",
    "\n",
    "        token_onehot = decoder_output[0, count_tokens, :]\n",
    "        token_int = np.argmax(token_onehot)\n",
    "        \n",
    "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
    "        output_text += \" \" + sampled_word\n",
    "        count_tokens += 1\n",
    "\n",
    "    \n",
    "    print(\"Input text:\")\n",
    "    print(input_text)\n",
    "    print()\n",
    "\n",
    "    print(\"Translated text:\")\n",
    "    print(output_text)\n",
    "    print()\n",
    "\n",
    "    if true_output_text is not None:\n",
    "        print(\"True output text:\")\n",
    "        print(true_output_text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Öncelikle verisetinde bulunan cümlelerin çeviri örneklerine bakalım. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "Do you know my name?\n",
      "\n",
      "Translated text:\n",
      " adımı biliyor musun eeee\n",
      "\n",
      "True output text:\n",
      "ssss Adımı biliyor musun? eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(input_text=data_src[50000], true_output_text=data_dest[50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "I hope so.\n",
      "\n",
      "Translated text:\n",
      " öyle umuyorum eeee\n",
      "\n",
      "True output text:\n",
      "ssss Öyle olduğunu umuyorum. eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(input_text=data_src[500], true_output_text=data_dest[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "I lost.\n",
      "\n",
      "Translated text:\n",
      " kaybettim eeee\n",
      "\n",
      "True output text:\n",
      "ssss Kayboldum. eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(input_text=data_src[40], true_output_text=data_dest[40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "Tom will certainly try to do that.\n",
      "\n",
      "Translated text:\n",
      " tom kesinlikle bunu yapmaya çalışacak eeee\n",
      "\n",
      "True output text:\n",
      "ssss Tom kesinlikle onu yapmaya çalışacak. eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(input_text=data_src[300000], true_output_text=data_dest[300000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "You are telling it second hand, aren't you?\n",
      "\n",
      "Translated text:\n",
      " onu ikinci balık söylüyorsun değil mi eeee\n",
      "\n",
      "True output text:\n",
      "ssss Onu dolaylı olarak anlatıyorsun, değil mi? eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(input_text=data_src[400000], true_output_text=data_dest[400000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daha sonra verisetinde olmayan cümleler ile modeli test edelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "This summer I went to Japan\n",
      "\n",
      "Translated text:\n",
      " bu yaz japonya'ya gittim eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(input_text=\"This summer I went to Japan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "Where did you buy this dress?\n",
      "\n",
      "Translated text:\n",
      " bu elbiseyi nereden aldın eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(input_text=\"Where did you buy this dress?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "Which road leads to the airport?\n",
      "\n",
      "Translated text:\n",
      " hangi yol havaalanına gider eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(input_text=\"Which road leads to the airport?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sonuç:\n",
    "\n",
    "Sonuç olarak basit cümleleri başarılı bir şekilde çevirebilen bir model elde ettik. Hem modelin küçük olmasından hem de verisetinin yetersiz olmasından dolayı model biraz daha karmaşık cümleleri çevirmekte zorluk yaşıyor. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
